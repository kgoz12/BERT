{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = \"/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade pip\n",
    "# %pip install transformers\n",
    "# %pip install tensorflow_datasets\n",
    "# %pip install tensorflow\n",
    "# %pip install keras\n",
    "# %pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION A:\n",
    "# USE HUGGING FACE TO:\n",
    "# 1. USE ONE OF THEIR VOCABULARIES - BUT! - ADD THE RADIOLOGIST WORDS TO THAT VOCAB: add_tokens method for transformers.SpecialTokensMixin\n",
    "# 2. AFTER I EXTEND THE VOCAB, FINE TUNE ONE OF THE EXISTING MODELS\n",
    "# THE ONLY OTHER OPTION WOULD BE TO TRAIN BOTH A NEW TOKENIZER AND A NEW MODEL FROM SCRATCH\n",
    "\n",
    "# import pandas as pd\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION B:\n",
    "# USE HUGGING FACE TO:\n",
    "# 1. CREATE MY OWN VOCABULARY USING TXT FILES \n",
    "# 2. TRAIN A NEW BERT MODEL\n",
    "\n",
    "# this uses an example published by hugging face\n",
    "from tokenizers import ByteLevelBPETokenizer, CharBPETokenizer, SentencePieceBPETokenizer, BertWordPieceTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "paths = [str(x) for x in Path(\"./esperanto/\").glob(\"**/*.txt\")]\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(files = paths, vocab_size = 52_000, min_frequency = 2, special_tokens = [\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./esperanto_vocab/esperberto-vocab.json',\n",
       " './esperanto_vocab/esperberto-merges.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_model(\"./esperanto_vocab/\",\"esperberto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "['<s>', 'Mi', 'Ä estas', 'Ä Juli', 'en', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "# now we want to use the saved vocabulary\n",
    "tokenizer = ByteLevelBPETokenizer(\"./esperanto_vocab/esperberto-vocab.json\",\n",
    "                                 \"./esperanto_vocab/esperberto-merges.txt\")\n",
    "\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")), \n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "\n",
    "tokenizer.enable_truncation(max_length = 512)\n",
    "\n",
    "print(tokenizer.encode(\"Mi estas Julien.\"))\n",
    "print(tokenizer.encode(\"Mi estas Julien.\").tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of creating esperanto dataframe as below can I use the technique in IMDB example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EsperantoDataset(Dataset):\n",
    "    \"\"\"Make Esperanto Dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        tokenizer = ByteLevelBPETokenizer(\n",
    "            \"./esperanto_vocab/esperberto-vocab.json\",\n",
    "            \"./esperanto_vocab/esperberto-merges.txt\",\n",
    "        )\n",
    "        tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "            (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "            (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "        )\n",
    "        tokenizer.enable_truncation(max_length=512) \n",
    "\n",
    "        self.examples = []\n",
    "\n",
    "        src_files = Path(\"./esperanto/\").glob(\"**/*.txt\")\n",
    "        for src_file in src_files:\n",
    "            lines = src_file.read_text(encoding=\"utf-8\").splitlines()\n",
    "            self.examples += [x.ids for x in tokenizer.encode_batch(lines)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return torch.tensor(self.examples[i])\n",
    "    \n",
    "espDS = EsperantoDataset()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "974616\n",
      "[0, 14488, 373, 5811, 274, 8634, 4616, 16, 5505, 274, 18297, 43129, 428, 10677, 288, 4729, 296, 1410, 288, 1259, 313, 18, 1130, 316, 27025, 1072, 428, 6473, 288, 6531, 274, 23496, 44134, 296, 752, 31297, 16, 1282, 2853, 316, 25989, 1122, 376, 4445, 12086, 31668, 288, 37105, 16, 1485, 274, 18297, 43129, 18, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "__main__.EsperantoDataset"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# src_files = Path(\"./esperanto/\").glob(\"**/*.txt\")\n",
    "# for src_file in src_files:\n",
    "#     kmg = src_file.read_text(encoding = \"utf-8\").splitlines()\n",
    "# thing = tokenizer.encode_batch(kmg)\n",
    "# print(thing[1].tokens)\n",
    "\n",
    "print(len(espDS))\n",
    "print(espDS.examples[1])\n",
    "type(espDS)\n",
    "# so each record in the txt file has been tokenized and the tokens have been replaced by the id number in the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# train my own model\n",
    "from transformers import BertTokenizer#, glue_convert_examples_to_features\n",
    "import tensorflow_datasets as tfds\n",
    "from transformers import BertForMaskedLM, Trainer, TrainingArguments\n",
    "from transformers.data.processors.utils import DataProcessor, InputExample, InputFeatures\n",
    "\n",
    "# train_dataset = thing\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-large-uncased\") # this lets us start off w/ saved weights\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./esperanto_results',# output directory\n",
    "    num_train_epochs=3,              # total # of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "#     args=training_args,                  # training arguments, defined above\n",
    "#     train_dataset=train_dataset#,         # training dataset\n",
    "# #     eval_dataset=test_dataset            # evaluation dataset\n",
    "# )\n",
    "\n",
    "# # to train\n",
    "# trainer.train() \n",
    "\n",
    "# # to evaluate\n",
    "# trainer.evaluate() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "src_files = Path(\"./esperanto/\").glob(\"**/*.txt\")\n",
    "for src_file in src_files:\n",
    "    kmg = src_file.read_text(encoding = \"utf-8\").splitlines()\n",
    "\n",
    "def tokenize_sentences(sentences, tokenizer):\n",
    "    tokenized_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokenized_sentence = tokenizer.encode(\n",
    "                            sentence,                  # Sentence to encode.\n",
    "                            add_special_tokens = True # Add '[CLS]' and '[SEP]'\n",
    "                    )\n",
    "        \n",
    "        tokenized_sentences.append(tokenized_sentence)\n",
    "\n",
    "    return tokenized_sentences\n",
    "\n",
    "def create_attention_masks(tokenized_and_padded_sentences):\n",
    "    attention_masks = []\n",
    "\n",
    "    for sentence in tokenized_and_padded_sentences:\n",
    "        att_mask = [int(token_id > 0) for token_id in sentence]\n",
    "        attention_masks.append(att_mask)\n",
    "\n",
    "    return np.asarray(attention_masks)\n",
    "\n",
    "input_ids = tokenize_sentences(kmg, tokenizer)\n",
    "# attention_masks = create_attention_masks(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "<built-in method char_to_token of tokenizers.Encoding object at 0x7ff19a63c030>\n",
      "<built-in method char_to_word of tokenizers.Encoding object at 0x7ff19a63c030>\n",
      "[0, 14488, 373, 5811, 274, 8634, 4616, 16, 5505, 274, 18297, 43129, 428, 10677, 288, 4729, 296, 1410, 288, 1259, 313, 18, 1130, 316, 27025, 1072, 428, 6473, 288, 6531, 274, 23496, 44134, 296, 752, 31297, 16, 1282, 2853, 316, 25989, 1122, 376, 4445, 12086, 31668, 288, 37105, 16, 1485, 274, 18297, 43129, 18, 2]\n",
      "<built-in method merge of type object at 0x1062c5e00>\n",
      "[(0, 0), (0, 5), (5, 9), (9, 17), (17, 20), (20, 30), (30, 37), (37, 38), (38, 47), (47, 50), (50, 56), (56, 66), (66, 72), (72, 77), (77, 81), (81, 86), (86, 89), (89, 93), (93, 97), (97, 100), (100, 103), (103, 104), (104, 108), (108, 114), (114, 122), (122, 126), (126, 132), (132, 137), (137, 141), (141, 146), (146, 149), (149, 158), (158, 167), (167, 170), (170, 174), (174, 182), (182, 183), (183, 188), (188, 193), (193, 199), (199, 210), (210, 215), (215, 219), (219, 223), (223, 229), (229, 236), (236, 240), (240, 251), (251, 252), (252, 257), (257, 260), (260, 266), (266, 276), (276, 277), (0, 0)]\n",
      "[]\n",
      "<built-in method pad of tokenizers.Encoding object at 0x7ff19a63c030>\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "<built-in method token_to_chars of tokenizers.Encoding object at 0x7ff19a63c030>\n",
      "<built-in method token_to_word of tokenizers.Encoding object at 0x7ff19a63c030>\n",
      "['<s>', 'Temas', 'Ä pri', 'Ä kolekto', 'Ä de', 'Ä kristanaj', 'Ä kantoj', ',', 'Ä eldonita', 'Ä de', 'Ä Adolf', 'Ä Burkhardt', 'Ä inter', 'Ä 1974', 'Ä kaj', 'Ä 1990', 'Ä en', 'Ä dek', 'Ä kaj', 'ere', 'toj', '.', 'Ä Ili', 'Ä estas', 'Ä reeldon', 'itaj', 'Ä inter', 'Ä 1995', 'Ä kaj', 'Ä 1998', 'Ä de', 'Ä Bernhard', 'Ä Eichkorn', 'Ä en', 'Ä tri', 'Ä kajeroj', ',', 'Ä kies', 'Ä tria', 'Ä estas', 'Ä pliampleks', 'igita', 'Ä per', 'Ä Dek', 'Ä Novaj', 'Ä Kantoj', 'Ä kaj', 'Ä suplemento', ',', 'Ä same', 'Ä de', 'Ä Adolf', 'Ä Burkhardt', '.', '</s>']\n",
      "<built-in method truncate of tokenizers.Encoding object at 0x7ff19a63c030>\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<built-in method word_to_chars of tokenizers.Encoding object at 0x7ff19a63c030>\n",
      "<built-in method word_to_tokens of tokenizers.Encoding object at 0x7ff19a63c030>\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 17, 17, 18, 19, 20, 21, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, None]\n"
     ]
    }
   ],
   "source": [
    "# dir(input_ids[1])\n",
    "print(input_ids[1].attention_mask)\n",
    "print(input_ids[1].char_to_token)\n",
    "print(input_ids[1].char_to_word)\n",
    "print(input_ids[1].ids)\n",
    "print(input_ids[1].merge)\n",
    "print(input_ids[1].offsets)\n",
    "print(input_ids[1].overflowing)\n",
    "print(input_ids[1].pad)\n",
    "print(input_ids[1].special_tokens_mask)\n",
    "print(input_ids[1].token_to_chars)\n",
    "print(input_ids[1].token_to_word)\n",
    "print(input_ids[1].tokens)\n",
    "print(input_ids[1].truncate)\n",
    "print(input_ids[1].type_ids)\n",
    "print(input_ids[1].word_to_chars)\n",
    "print(input_ids[1].word_to_tokens)\n",
    "print(input_ids[1].words)\n",
    "\n",
    "# def create_dataset(ids, masks, labels):\n",
    "#     def gen():\n",
    "#         for i in range(len(train_ids)):\n",
    "#             yield (\n",
    "#                 {\n",
    "#                     \"input_ids\": ids[i],\n",
    "#                     \"attention_mask\": masks[i]\n",
    "#                 },\n",
    "#                 labels[i],\n",
    "#             )\n",
    "\n",
    "#     return tf.data.Dataset.from_generator(\n",
    "#         gen,\n",
    "#         ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32}, tf.int64),\n",
    "#         (\n",
    "#             {\n",
    "#                 \"input_ids\": tf.TensorShape([None]),\n",
    "#                 \"attention_mask\": tf.TensorShape([None])\n",
    "#             },\n",
    "#             tf.TensorShape([None]),\n",
    "#         ),\n",
    "#     )\n",
    "\n",
    "# train_dataset = create_dataset(train_ids, train_masks, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# fill_mask = pipeline(\n",
    "#     \"fill-mask\", \n",
    "#     model = \"./models/EsperBERTo-small\", \n",
    "#     tokenizer = \"./models/EsperBERTo-small\"\n",
    "# )\n",
    "\n",
    "# result = fill_mask(\"La suno <mask>.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
