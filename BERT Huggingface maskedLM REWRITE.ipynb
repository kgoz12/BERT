{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://mccormickml.com/2019/07/22/BERT-fine-tuning/#42-optimizer--learning-rate-scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import DataLoader\n",
    "import torch \n",
    "from tqdm import tqdm\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import pipeline, BertTokenizerFast, BertTokenizer, BertForMaskedLM #AutoTokenizer, AutoModelWithLMHead\n",
    "from nlp import load_dataset\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = \"/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade pip\n",
    "# %pip install --upgrade torch\n",
    "# %pip install --upgrade tensorflow\n",
    "# %pip install --upgrade pathlib\n",
    "# %pip install --upgrade --use-feature=2020-resolver tokenizers\n",
    "# %pip install --upgrade --use-feature=2020-resolver transformers # resolve needed so no version conflicts\n",
    "# %pip install --upgrade nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the train/text IMDB reviews\n",
    "def read_imdb_split(split_dir):\n",
    "    split_dir = Path(split_dir)\n",
    "    texts = []\n",
    "    for label_dir in [\"pos\", \"neg\"]:\n",
    "        for text_file in (split_dir/label_dir).iterdir():\n",
    "            texts.append(text_file.read_text())\n",
    "\n",
    "    return texts\n",
    "\n",
    "train_texts = read_imdb_split('./aclImdb/train')\n",
    "test_texts = read_imdb_split('./aclImdb/test')\n",
    "\n",
    "# and write these out to a single text file (new line for each review)\n",
    "# write the list of review to text files\n",
    "texts = train_texts + test_texts\n",
    "\n",
    "# delete whatever is in the dataset folder\n",
    "files = glob.glob('./aclImdbKate/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "# write each chunk of text to a single txt file \n",
    "for text in texts:\n",
    "    with open('./aclImdbKate/text.txt', 'a+', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "        file.write(\"\\n\") # go to new line at end of each body of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./aclImdbKateVocab/imdb-bert-wordpiece-vocab.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make my own tokenizer\n",
    "paths = paths = [str(x) for x in Path(\"./aclImdbKate/\").glob(\"**/*.txt\")]\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "\n",
    "tokenizer.enable_truncation(max_length = 512)\n",
    "\n",
    "tokenizer.train(files = paths, \n",
    "                vocab_size = 5_000, \n",
    "                min_frequency = 2,\n",
    "                special_tokens = ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']\n",
    "               )\n",
    "\n",
    "tokenizer.save_model(\"./aclImdbKateVocab/\", name = 'imdb-bert-wordpiece')\n",
    "\n",
    "# print(tokenizer.truncation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nlp.arrow_dataset.Dataset'>\n",
      "(50000, 1)\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "# TODO: I need to find a way to separate each sentence to be on its own line...so sentences get tokenized correctly\n",
    "# (start with [CLS] and end with [SEP])\n",
    "\n",
    "# load my dataset\n",
    "dataset = load_dataset(\n",
    "    'text', \n",
    "    data_files={'train': ['./aclImdbKate/text.txt']}, \n",
    "    split='train')\n",
    "\n",
    "print(type(dataset))\n",
    "print(dataset.shape)\n",
    "# print(dataset.column_names)\n",
    "# print(dataset.features)\n",
    "# print(dataset[0])\n",
    "# print(dataset[1])\n",
    "\n",
    "# 50K records are too many to process locally\n",
    "dataset = dataset.filter(lambda e, i: i<1000, with_indices=True)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# tell pytorch what kind of processor is available to it\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up BERT tokenizer with the vocab I defined in another script\n",
    "tokenizer = BertTokenizer(vocab_file = './aclImdbKateVocab/imdb-bert-wordpiece-vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['attention_mask', 'input_ids', 'labels', 'text', 'token_type_ids']\n",
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [2, 239, 43, 260, 225, 908, 388, 2076, 339, 919, 280, 43, 682, 199, 2484, 467, 4092, 1558, 195, 239, 224, 3126, 18, 2337, 43, 260, 583, 2585, 58, 3335, 311, 133, 207, 750, 734, 5, 301, 448, 190, 227, 446, 2479, 194, 207, 43, 637, 4450, 184, 18, 183, 1152, 194, 127, 553, 207, 242, 1310, 2865, 18, 389, 239, 4155, 6, 183, 4237, 520, 6, 1018, 124, 3992, 18, 241, 43, 1818, 61, 135, 131, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [2, 239, 43, 260, 225, 908, 388, 2076, 339, 919, 280, 43, 682, 199, 2484, 467, 4092, 1558, 195, 239, 224, 3126, 18, 2337, 43, 260, 583, 2585, 58, 3335, 311, 133, 207, 750, 734, 5, 301, 448, 190, 227, 446, 2479, 194, 207, 43, 637, 4450, 184, 18, 183, 1152, 194, 127, 553, 207, 242, 1310, 2865, 18, 389, 239, 4155, 6, 183, 4237, 520, 6, 1018, 124, 3992, 18, 241, 43, 1818, 61, 135, 131, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'text': 'For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt.\\n', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "[2, 239, 43, 260, 225, 908, 388, 2076, 339, 919, 280, 43, 682, 199, 2484, 467, 4092, 1558, 195, 239, 224, 3126, 18, 2337, 43, 260, 583, 2585, 58, 3335, 311, 133, 207, 750, 734, 5, 301, 448, 190, 227, 446, 2479, 194, 207, 43, 637, 4450, 184, 18, 183, 1152, 194, 127, 553, 207, 242, 1310, 2865, 18, 389, 239, 4155, 6, 183, 4237, 520, 6, 1018, 124, 3992, 18, 241, 43, 1818, 61, 135, 131, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[CLS] for a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. imagine a movie where joe piscopo is actually funny! maureen stapleton is a scene stealer. the moroni character is an absolute scream. watch for alan \" the skipper \" hale jr. as a police sgt. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# encode my dataset \n",
    "# (adds the ids for vocab words, indicated whether the end of the text chunk contains padding)\n",
    "\n",
    "encoded_dataset = dataset.map(\n",
    "    lambda examples: tokenizer(\n",
    "        dataset['text'],\n",
    "        truncation=True,\n",
    "        max_length = 128, # too low for my real data\n",
    "        padding=True,\n",
    "        return_attention_mask = True\n",
    "    ), \n",
    "    batch_size = 1000, # this is the max number of rows in your dataset that's allowed.\n",
    "    batched=True)\n",
    "\n",
    "# add a labels column, and that column needs to be set to the input_ids\n",
    "encoded_dataset = encoded_dataset.map(\n",
    "    lambda example: {'labels': example['input_ids']})\n",
    "\n",
    "print(encoded_dataset.column_names)\n",
    "print(encoded_dataset[0])\n",
    "print(encoded_dataset['input_ids'][0])\n",
    "\n",
    "print(tokenizer.decode(tokenizer(dataset[0]['text'])['input_ids'])) \n",
    "# note that [SEP] token doesn't appear at end of each sentence, that's what I want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the encoded data to a tensorflow type via DataLoader\n",
    "encoded_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "dataloader = DataLoader(encoded_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/nlp/utils/py_utils.py:191: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return function(data_struct)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]),\n",
       " 'input_ids': tensor([[   2,  239,   43,  ...,    0,    0,    0],\n",
       "         [   2, 2899,  819,  ...,  222,  183,    3],\n",
       "         [   2,   43, 2897,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [   2,  224,  207,  ...,   11,  914,    3],\n",
       "         [   2,  224, 1468,  ...,   19, 1503,    3],\n",
       "         [   2,   51,  240,  ...,  198,  389,    3]]),\n",
       " 'labels': tensor([[   2,  239,   43,  ...,    0,    0,    0],\n",
       "         [   2, 2899,  819,  ...,  222,  183,    3],\n",
       "         [   2,   43, 2897,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [   2,  224,  207,  ...,   11,  914,    3],\n",
       "         [   2,  224, 1468,  ...,   19, 1503,    3],\n",
       "         [   2,   51,  240,  ...,  198,  389,    3]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dir(dataloader)\n",
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # start the steps needed to fine tune a model\n",
    "\n",
    "# # this pulls the architecture of one of the existing models\n",
    "# model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "# model.to(device) # recall this is cpu as defined in above cell\n",
    "# model.train() # this changes the model's \"mode\", it does not perform the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this does a pretty print-out of model architecture\n",
    "# # TODO: confirm that nothing is being left out of this print-out\n",
    "# params = list(model.named_parameters())\n",
    "\n",
    "# print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "# print('==== Embedding Layer ====\\n')\n",
    "# for p in params[0:5]:\n",
    "#     print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "# print('\\n==== First Transformer ====\\n')\n",
    "# for p in params[5:21]:\n",
    "#     print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "# print('\\n==== Output Layer ====\\n')\n",
    "# for p in params[-4:]:\n",
    "#     print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "    \n",
    "# 30522 is the length of original vocab. BUT, my vocab is smaller. So, cannot fine-tune, need to retrain completely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Not entirely clear on this, looks like the manager that hold the tuning params/model architecture for training\n",
    "# optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)\n",
    "\n",
    "# for epoch in range(2):\n",
    "#     for i, batch in enumerate(tqdm(dataloader)):\n",
    "#         batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#         outputs = model(**batch)\n",
    "#         loss = outputs[0]\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "#         if i % 10 == 0:\n",
    "#             print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save my model with new weights - the weight have been re-fitted, right?\n",
    "# model.save_pretrained('./aclImdbKateModel/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill_mask = pipeline(\n",
    "#     \"fill-mask\", \n",
    "#     model = model, \n",
    "#     tokenizer = tokenizer\n",
    "# )\n",
    "\n",
    "# result = fill_mask('This is one of my favorite [MASK] .')\n",
    "# print(result) # TODO: fill mask pipeline not performing as expected..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "# # change tokenizer and model to some pretrained...\n",
    "# tokenizer1 = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model1 = AutoModelWithLMHead.from_pretrained(\"bert-base-uncased\") \n",
    "\n",
    "# fill_mask = pipeline(\n",
    "#     \"fill-mask\", \n",
    "#     model = model1, \n",
    "#     tokenizer = tokenizer1\n",
    "# )\n",
    "\n",
    "# result = fill_mask('This is one of my favorite [MASK] .')\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4dc3ffb325d4ddc88a4d6c17daecada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=2.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d65bbfbd54342fe9add731123f9b2ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=63.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e46c66d4a39495ba3b3c23f51aa36b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=63.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=126, training_loss=2.802249484592014)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We might need to use Trainer and TrainingArguments to train FROM SCRATCH!\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import BertConfig \n",
    "\n",
    "config = BertConfig(vocab_size = tokenizer.vocab_size)  \n",
    "model = BertForMaskedLM(config)\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/a75c64d80c76c3dc71f735d9197a4a601847e0cd/src/transformers/training_args.py\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './results',          # output directory\n",
    "    overwrite_output_dir = True,       # whether the contents in the output dir should be overwritten\n",
    "    do_train = True,                   # why does this default to False? What the heck are you running this for if not to train a new model??\n",
    "    num_train_epochs = 2,              # total number of training epochs (2-4, rec)\n",
    "    per_device_train_batch_size = 16,  # batch size per device during training\n",
    "#     per_device_eval_batch_size = 64,   # batch size for evaluation\n",
    "    warmup_steps = 0,                  # number of warmup steps for learning rate scheduler\n",
    "    weight_decay = 0.01,               # strength of weight decay\n",
    "    logging_dir = './logs',            # directory for storing logs\n",
    "    seed = 123                         # random seed\n",
    ")\n",
    "\n",
    "tokenizer = BertTokenizer(vocab_file = './aclImdbKateVocab/imdb-bert-wordpiece-vocab.txt')\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/a75c64d80c76c3dc71f735d9197a4a601847e0cd/src/transformers/trainer.py\n",
    "trainer = Trainer(\n",
    "    args = training_args,              # training arguments, defined above\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset=encoded_dataset,     # training dataset (torch.utils.data.dataset.Dataset or nlp.Dataset?)\n",
    "    model = model     \n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 204 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                   (5000, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "cls.predictions.transform.dense.weight                    (768, 768)\n",
      "cls.predictions.transform.dense.bias                          (768,)\n",
      "cls.predictions.transform.LayerNorm.weight                    (768,)\n",
      "cls.predictions.transform.LayerNorm.bias                      (768,)\n"
     ]
    }
   ],
   "source": [
    "# this does a pretty print-out of model architecture\n",
    "# TODO: confirm that nothing is being left out of this print-out\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "print('==== Embedding Layer ====\\n')\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./aclImdbKateModel/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sequence': '[CLS] this is one of my favorite br. [SEP]', 'score': 0.005493552889674902, 'token': 206, 'token_str': 'br'}, {'sequence': '[CLS] this is one of my favorite [MASK]. [SEP]', 'score': 0.004160965792834759, 'token': 4, 'token_str': '[MASK]'}, {'sequence': '[CLS] this is one of my favorite european. [SEP]', 'score': 0.0014475906500592828, 'token': 4434, 'token_str': 'european'}, {'sequence': '[CLS] this is one of my favorite ). [SEP]', 'score': 0.0013414985733106732, 'token': 13, 'token_str': ')'}, {'sequence': '[CLS] this is one of my favoriteir. [SEP]', 'score': 0.0013259300030767918, 'token': 258, 'token_str': '##ir'}]\n"
     ]
    }
   ],
   "source": [
    "fill_mask = pipeline(\n",
    "    \"fill-mask\", \n",
    "    model = model, \n",
    "    tokenizer = tokenizer\n",
    ")\n",
    "\n",
    "result = fill_mask('This is one of my favorite [MASK] .')\n",
    "print(result) # TODO: make it so the results of this pipeline are reproducible...do I need to \"take model out of train mode?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence = f\"Hugging Face is a French company based in {tokenizer.mask_token}\"\n",
    "\n",
    "# input_ids = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "# mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
    "\n",
    "# token_logits = model(input_ids)[0]\n",
    "# mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# mask_token_logits = torch.softmax(mask_token_logits, dim=1)\n",
    "\n",
    "# top_5 = torch.topk(mask_token_logits, 5, dim=1)\n",
    "# top_5_tokens = zip(top_5.indices[0].tolist(), top_5.values[0].tolist())\n",
    "\n",
    "# for token, score in top_5_tokens:\n",
    "#     print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])), f\"(score: {score})\")\n",
    "\n",
    "# # Get the score of token_id\n",
    "# sought_after_token = \"London\"\n",
    "# sought_after_token_id = tokenizer.encode(sought_after_token, add_special_tokens=False, add_prefix_space=True)[0]  # 928\n",
    "\n",
    "# token_score = mask_token_logits[:, sought_after_token_id]\n",
    "# print(f\"Score of {sought_after_token}: {mask_token_logits[:, sought_after_token_id]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
