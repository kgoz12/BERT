{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = \"C:\\Java\\jdk1.8.0_221\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install torch \n",
    "# !{sys.executable} -m pip install --upgrade tensorflow\n",
    "# !{sys.executable} -m pip install --upgrade transformers \n",
    "# !{sys.executable} -m pip install --upgrade tokenizers\n",
    "# !{sys.executable} -m pip install --upgrade datasets\n",
    "# !{sys.executable} -m pip install --upgrade nltk\n",
    "# !{sys.executable} -m pip install bert_score\n",
    "# !{sys.executable} -m pip install seaborn\n",
    "# !{sys.executable} -m pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import csv \n",
    "\n",
    "import torch \n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertForMaskedLM, DistilBertConfig, pipeline, LineByLineTextDataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the eval dataframe so we can determine probabilities of each word being the likely word for that sentence.\n",
    "eval_data = pd.read_csv(\n",
    "    './telemed_Data_Evaluate_Big/sentence_data.csv',\n",
    "#     './telemed_Data_Evaluate/sentence_data.csv', \n",
    "    sep='\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the tokenizer \n",
    "tokenizer = DistilBertTokenizer('./telemed_Vocab/telemed-bert-wordpiece-vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the fine tuned model\n",
    "config = DistilBertConfig(\n",
    "    vocab_size = tokenizer.vocab_size, # this is the only default I'm changing\n",
    "    max_position_embeddings=512, \n",
    "    sinusoidal_pos_embds=False, \n",
    "    n_layers=6, \n",
    "    n_heads=12, \n",
    "    dim=768, \n",
    "    hidden_dim=3072, \n",
    "    dropout=0.1, \n",
    "    attention_dropout=0.1, \n",
    "    activation='gelu', \n",
    "    initializer_range=0.02, \n",
    "    qa_dropout=0.1, \n",
    "    seq_classif_dropout=0.2, \n",
    "    pad_token_id=0\n",
    ")\n",
    "\n",
    "model = DistilBertForMaskedLM(config)\n",
    "\n",
    "modelTrained = model.from_pretrained('./telemed_Model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask = pipeline('fill-mask', './telemed_Fill_Mask_Pipeline/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # There are no appreciable mineral opaque calculi within the urinary bladder on the available study.\n",
    "# sequence = f\"There are no appreciable mineral opaque {fill_mask.tokenizer.mask_token} within the urinary bladder on the available study.\"\n",
    "# top_k = fill_mask(sequence)\n",
    "\n",
    "# print(\"Complete sentence: There are no appreciable mineral opaque calculi within the urinary bladder on the available study.\")\n",
    "# print(\"\\n\")\n",
    "# print(\"Sentence with exactly one token masked:\")\n",
    "# print(sequence)\n",
    "# print(\"\\n\")\n",
    "# print(\"k sentences with masked token filled and likelihood (probability?) of that token:\")\n",
    "# for item in top_k:\n",
    "#     print(item['sequence'])\n",
    "#     print(item['score'])\n",
    "# #     print(item['token'])\n",
    "# #     print(item['token_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # try another sentence \n",
    "# # sequence = f\"There is a mild diffuse {tokenizer.mask_token} she will long pattern, the interstitial component of which is accentuated by expiratory phase of respiration.\"\n",
    "# sequence = f\"There is a mild diffuse {fill_mask.tokenizer.mask_token} lung pattern, the interstitial component of which is accentuated by expiratory phase of respiration.\" \n",
    "# # top_k = fill_mask(sequence)\n",
    "\n",
    "# # print(\"Complete sentence: There is a mild diffuse broncho-interspace she will long pattern, the interstitial component of which is accentuated by expiratory phase of respiration.\")\n",
    "# # print(\"\\n\")\n",
    "# # print(\"Sentence with exactly one token masked:\")\n",
    "# print(sequence)\n",
    "# print(tokenizer(sequence))\n",
    "# fill_mask(sequence)\n",
    "# # print(\"\\n\")\n",
    "# # print(\"k sentences with masked token filled and likelihood (probability?) of that token:\")\n",
    "# # for item in top_k:\n",
    "# #     print(item['sequence'])\n",
    "# #     print(item['score'])\n",
    "    \n",
    "# # parenchymal is an adjective!!\n",
    "# # parenchyma is a noun - model knows a noun belongs in that slot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # try another sentence \n",
    "# # sequence = f\"Since the previous exam, there has been {tokenizer.mask_token} improvement in the previously described bronchointerstitial to alveolar lung pattern.\" \n",
    "# sequence = f\"Since the previous exam, there has been {fill_mask.tokenizer.mask_token} improvement in the previously described bronchointerstitial to alveolar lung pattern.\" \n",
    "\n",
    "# top_k = fill_mask(sequence, targets = ['substantial'])\n",
    "\n",
    "# print(\"Complete sentence: Since the previous exam, there has been substantial improvement in the previously described bronchointerstitial to alveolar lung pattern.\")\n",
    "# print(\"\\n\")\n",
    "# print(\"Sentence with exactly one token masked:\")\n",
    "# print(sequence)\n",
    "# print(\"\\n\")\n",
    "# print(\"k sentences with masked token filled and likelihood (probability?) of that token:\")\n",
    "# for item in top_k:\n",
    "#     print(item['sequence'])\n",
    "#     print(item['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can create a mini UI \n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# instantiate some output\n",
    "out = widgets.Output()\n",
    "\n",
    "# make the text box sufficiently wide\n",
    "layout = widgets.Layout(width='auto') \n",
    " \n",
    "# define characteristics of text box    \n",
    "input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter sentence',\n",
    "    description='Sentence:',\n",
    "    layout = layout,\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "def forward(_):\n",
    "    # clear out the old output\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "    # if the user has entered some text then do the following    \n",
    "    if len(input.value) > 0:\n",
    "        sentence_text = input.value\n",
    "        token_id_list = tokenizer(sentence_text).input_ids\n",
    "        token_list = tokenizer.convert_ids_to_tokens(token_id_list, skip_special_tokens = True)\n",
    "        mask_text = \" \" + tokenizer.mask_token + \" \"\n",
    "        for x in range(len(token_list)):\n",
    "            sentence_with_one_mask = tokenizer.convert_tokens_to_string(token_list[0:x]) +  mask_text  + tokenizer.convert_tokens_to_string(token_list[(x+1):len(token_list)])\n",
    "            mask_fill_output = fill_mask(sentence_with_one_mask, targets = token_list[x])\n",
    "            actual_token = mask_fill_output[0]['token_str']\n",
    "            actual_token_prob = mask_fill_output[0]['score']\n",
    "            mask_fill_output_top1 = fill_mask(sentence_with_one_mask)\n",
    "            with out:\n",
    "                print(\"Sentence w/ 1 token masked: \" + sentence_with_one_mask)\n",
    "                print(\"Actual word: \" + actual_token)\n",
    "                print(\"Probability of actual word: \" + str(actual_token_prob))\n",
    "                print(\"Most likely word: \" + mask_fill_output_top1[0]['token_str'])\n",
    "                print(\"Probability of most likely word: \" + str(mask_fill_output_top1[0]['score']))\n",
    "                print('\\n')\n",
    "        \n",
    "input.on_submit(forward)\n",
    "display(input, out)\n",
    "\n",
    "# after you paste input just hit return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write a function that sequentially masks each token in a sentence and then checks the topk tokens in fill-mask pipeline \n",
    "# # to see if one of those match the masked token\n",
    "\n",
    "# sentence_text = \"Since the previous exam, there has been substantial improvement in the previously described bronchointerstitial to alveolar lung pattern.\" \n",
    "\n",
    "# token_id_list = tokenizer(sentence_text).input_ids\n",
    "# token_list = tokenizer.convert_ids_to_tokens(token_id_list, skip_special_tokens = True)\n",
    "# print(token_list)\n",
    "# print('\\n')\n",
    "# mask_text = \" \" + tokenizer.mask_token + \" \"\n",
    "\n",
    "# # loop through all tokens in sentence\n",
    "# for x in range(len(token_list)):\n",
    "#     sentence_with_one_mask = tokenizer.convert_tokens_to_string(token_list[0:x]) +  mask_text  + tokenizer.convert_tokens_to_string(token_list[(x+1):len(token_list)])\n",
    "#     print(\"Sentence w/ 1 token masked: \" + sentence_with_one_mask)\n",
    "#     mask_fill_output = fill_mask(sentence_with_one_mask, targets = token_list[x])\n",
    "#     actual_token = mask_fill_output[0]['token_str']\n",
    "#     actual_token_prob = mask_fill_output[0]['score']\n",
    "#     print(actual_token + \": \" + str(actual_token_prob))\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this function returns the predicted most likely sentence\n",
    "# def predict(sentence):\n",
    "#     input_ids = torch.tensor(tokenizer.encode(sentence)).unsqueeze(0)\n",
    "#     outputs = modelTrained(input_ids, labels = input_ids)\n",
    "#     loss, prediction_scores = outputs[:2]\n",
    "#     print(prediction_scores.shape)\n",
    "#     text = ''\n",
    "#     for i in range(1, prediction_scores.shape[1]-1):\n",
    "#         t = np.argmax(prediction_scores[0, i].tolist())\n",
    "#         options = tokenizer.convert_ids_to_tokens([t])\n",
    "#         text = text + ' ' + options[0]  \n",
    "#     return text\n",
    "\n",
    "# print(predict(\"Since the previous exam, there has been substantial improvement in the previously described bronchointerstitial to alveolar lung pattern.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pred(sentence):\n",
    "#     input_ids = torch.tensor(tokenizer.encode(sentence)).unsqueeze(0)\n",
    "#     outputs = modelTrained(input_ids, labels = input_ids)\n",
    "#     loss, prediction_scores = outputs[:2] \n",
    "#     probabilities = torch.nn.functional.softmax(prediction_scores, dim = -1)\n",
    "#     for i in range(1, prediction_scores.shape[1]-1): # 1 to N-1 to skip [CLS] & [SEP]\n",
    "#         j = input_ids[0, i]\n",
    "#         p = probabilities[0, i, j].tolist()\n",
    "#         text = tokenizer.convert_ids_to_tokens([j])\n",
    "#         print(text[0] + \"     \" + str(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pred(\"Since the previous exam, there has been substantial improvement in the previously described bronchointerstitial to alveolar lung pattern.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use line by line function to bring in eval dataset\n",
    "linebyline_eval_data = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"./telemed_Data_Evaluate_Big/sentence_data.csv\",\n",
    "    block_size=128\n",
    ")\n",
    "\n",
    "# create an empty array in which probabilities of each actual word will be stored\n",
    "prob_array = np.empty((0,1), float)\n",
    "\n",
    "# print(len(linebyline_eval_data.examples))\n",
    "# print(linebyline_eval_data.examples[0]) # len = 200,000\n",
    "# print(type(linebyline_eval_data.examples))\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for example in tqdm(linebyline_eval_data.examples[:20_000]): #looping though all 200,000 rows takes forever\n",
    "    input_ids = torch.tensor(tokenizer.encode(example)).unsqueeze(0)\n",
    "    outputs = modelTrained(input_ids, labels = input_ids)\n",
    "    loss, prediction_scores = outputs[:2] \n",
    "    probabilities = torch.nn.functional.softmax(prediction_scores, dim = -1)\n",
    "    for i in range(1, prediction_scores.shape[1]-1): # 1 to N-1 to skip [CLS] & [SEP]\n",
    "        j = input_ids[0, i]\n",
    "        p = probabilities[0, i, j].tolist()\n",
    "        prob_array = np.append(prob_array, np.array([[p]]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prob_array[0])\n",
    "print(prob_array[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.distplot(prob_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((linebyline_eval_data.examples[:20_000][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the token id for backslash\n",
    "# token_id = tokenizer.convert_tokens_to_ids('\\\\')\n",
    "token_id = tokenizer.convert_tokens_to_ids(',')\n",
    "print(\"The ID of the token we want to find in text is: \" + str(token_id))\n",
    "\n",
    "# instantiate an empty list of the indexes of the above token\n",
    "token_index_list = []\n",
    "\n",
    "# convert list of lists to a numpy array    \n",
    "sentences_ids_array = numpy.array(linebyline_eval_data.examples[:7]) \n",
    "\n",
    "# # iterate through each list in array\n",
    "for list_obj in sentences_ids_array:\n",
    "    # find all indexes of the token_id in the sentence id list\n",
    "    index_list = list(np.where(np.isin(list_obj, [token_id]))[0])\n",
    "    # append above list to the overall list\n",
    "    token_index_list.append(index_list)\n",
    "    for index in index_list:\n",
    "        # find the words that precede and succeed it\n",
    "        print(tokenizer.convert_ids_to_tokens(list_obj[(index-2):(index+3)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
