{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = \"C:\\Java\\jdk1.8.0_221\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install --upgrade --use-feature=2020-resolver torch \n",
    "# !{sys.executable} -m pip install --upgrade --use-feature=2020-resolver tensorflow\n",
    "# !{sys.executable} -m pip install --upgrade --use-feature=2020-resolver transformers \n",
    "# !{sys.executable} -m pip install --use-feature=2020-resolver tokenizers\n",
    "# !{sys.executable} -m pip install --upgrade --use-feature=2020-resolver datasets\n",
    "# !{sys.executable} -m pip install --upgrade nltk\n",
    "# !{sys.executable} -m pip install bert_score\n",
    "# !{sys.executable} -m pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import csv \n",
    "\n",
    "import torch \n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DistilBertForMaskedLM, DistilBertConfig, DistilBertTokenizer\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "from datasets import  list_datasets, load_dataset, list_metrics, load_metric \n",
    "\n",
    "from nltk.tokenize import sent_tokenize # for splitting paragraphs into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS WORKAROUND FOR ISSUES WITH LOAD_DATASET\n",
    "import pandas as pd\n",
    "\n",
    "read_file = pd.read_csv(\n",
    "    './Desktop/telemed/data for Huggingface/huggingface_training_data.csv', \n",
    "    sep='\\n'\n",
    ")\n",
    "\n",
    "# convert text datatype to string\n",
    "read_file['text'] = read_file['text'].astype('string')\n",
    "\n",
    "# split the pandas data into separate sentences (from paragraphs)\n",
    "read_file['text'] = read_file['text'].apply(sent_tokenize)\n",
    "\n",
    "# then explode that dataframe so each sentence is in its own row\n",
    "read_file = read_file.explode('text')\n",
    "\n",
    "# convert text datatype to string\n",
    "read_file['text'] = read_file['text'].astype('string')\n",
    "\n",
    "# need to drop index or it created issues later\n",
    "read_file = read_file.reset_index(drop=True)\n",
    "\n",
    "# remove short strings\n",
    "read_file = read_file[read_file.text.str.len() > 1]\n",
    "\n",
    "# DELETE THIS - IT BAD\n",
    "read_file = read_file[0:100_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 1)\n",
      "(20000, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, evaluate = train_test_split(\n",
    "    read_file, \n",
    "    train_size=0.8, \n",
    "    random_state = 123\n",
    ")\n",
    "\n",
    "print(train.shape)\n",
    "print(evaluate.shape)\n",
    "\n",
    "# write training data and evaluation data to csv\n",
    "train.to_csv(\n",
    "    './telemed_Data_Train/sentence_data.csv', \n",
    "    index = False, \n",
    "    columns = ['text'],\n",
    "    header = False,\n",
    "    quoting = csv.QUOTE_NONE,\n",
    "    escapechar = '\\\\'\n",
    ")\n",
    "\n",
    "evaluate.to_csv(\n",
    "    './telemed_Data_Evaluate/sentence_data.csv', \n",
    "    index = False, \n",
    "    columns = ['text'],\n",
    "    header = False,\n",
    "    quoting = csv.QUOTE_NONE,\n",
    "    escapechar = '\\\\'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the file with each sentence on a separate line to csv\n",
    "read_file.to_csv(\n",
    "    './telemed_Data/sentence_data.csv', \n",
    "    index = False, \n",
    "    columns = ['text'],\n",
    "    header = False,\n",
    "    quoting = csv.QUOTE_NONE,\n",
    "    escapechar = '\\\\'\n",
    "#     quoting = csv.QUOTE_ALL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./telemed_Vocab/telemed-bert-wordpiece-vocab.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create vocab file....\n",
    "# paths = [str(x) for x in Path(\"./Desktop/telemed/data for Huggingface/\").glob(\"**/*.csv\")]\n",
    "paths = [str(x) for x in Path(\"./telemed_Data/\").glob(\"**/*.csv\")]\n",
    "tokenizer =  BertWordPieceTokenizer()\n",
    "\n",
    "tokenizer.enable_truncation(max_length = 512)\n",
    "tokenizer.train(files = paths, \n",
    "#                 vocab_size = 6000, # too big and we get weird results, too small and we get word pieces...what's just right\n",
    "                min_frequency = 2,\n",
    "                special_tokens = ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']\n",
    "               )\n",
    "tokenizer.save_model(\"./telemed_Vocab/\", name = 'telemed-bert-wordpiece')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer('./telemed_Vocab/telemed-bert-wordpiece-vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# tell pytorch what kind of processor is available to it\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.data.datasets.language_modeling.LineByLineTextDataset'>\n"
     ]
    }
   ],
   "source": [
    "# Is there a simpler way to build training dataset?\n",
    "datasetKMG = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"./telemed_Data/sentence_data.csv\",\n",
    "    block_size=128, # HOLY COW is the training fast! ... and block_size is max length of vector...so maybe dial up to 512? (but sentence len > 512? Not likely here.)\n",
    ")\n",
    "\n",
    "print(type(datasetKMG))\n",
    "# https://github.com/huggingface/transformers/blob/762cba3bdaf70104dc17cc7ff0f8ce13ba23d558/src/transformers/data/datasets/language_modeling.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build training dataset\n",
    "datasetTrain = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"./telemed_Data_Train/sentence_data.csv\",\n",
    "    block_size=128, # block_size is max length of vector...so maybe dial up to 512? (but sentence len > 512? Not likely here.)\n",
    ")\n",
    "\n",
    "# build evaluation dataset\n",
    "datasetEvaluate = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"./telemed_Data_Evaluate/sentence_data.csv\",\n",
    "    block_size=128, # block_size is max length of vector...so maybe dial up to 512? (but sentence len > 512? Not likely here.)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicitly create data collator instead of letting it get implicitly created by trainer\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer = tokenizer, \n",
    "    mlm = True, \n",
    "    mlm_probability = 0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 302, 28, 244, 4099, 35, 14, 321, 28, 21, 349, 134, 409, 16, 3]\n",
      "[2, 285, 28, 152, 122, 1570, 1316, 534, 364, 597, 682, 140, 981, 1214, 16, 3]\n",
      "[2, 109, 4422, 140, 3948, 1197, 2518, 134, 351, 1141, 4336, 140, 351, 712, 37, 838, 5262, 1641, 16, 3]\n",
      "[2, 109, 2088, 510, 140, 2400, 878, 120, 109, 841, 814, 35, 14, 1377, 140, 1693, 488, 16, 3]\n"
     ]
    }
   ],
   "source": [
    "print(datasetKMG.examples[0])\n",
    "print(datasetKMG.examples[1])\n",
    "print(datasetKMG.examples[2]) \n",
    "print(datasetKMG.examples[50003])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] abdomen : may 31 \\, 2020 : 3 images are provided. [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(datasetKMG.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasetTrain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-9681314e0abb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mdata_collator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_collator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mtrain_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasetTrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m#datasetKMG,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[0meval_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasetEvaluate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# TrainOutput(global_step=60000, training_loss=1.7492684895833333)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;31m#     compute_metrics = compute_metrics,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datasetTrain' is not defined"
     ]
    }
   ],
   "source": [
    "# We might need to use Trainer and TrainingArguments to train FROM SCRATCH!\n",
    "# https://github.com/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb\n",
    "\n",
    "config = DistilBertConfig(\n",
    "    vocab_size = tokenizer.vocab_size, # this is the only default I'm changing\n",
    "    max_position_embeddings=512, \n",
    "    sinusoidal_pos_embds=False, \n",
    "    n_layers=6, \n",
    "    n_heads=12, \n",
    "    dim=768, \n",
    "    hidden_dim=3072, \n",
    "    dropout=0.1, \n",
    "    attention_dropout=0.1, \n",
    "    activation='gelu', \n",
    "    initializer_range=0.02, \n",
    "    qa_dropout=0.1, \n",
    "    seq_classif_dropout=0.2, \n",
    "    pad_token_id=0\n",
    ")\n",
    "\n",
    "model = DistilBertForMaskedLM(config)\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/a75c64d80c76c3dc71f735d9197a4a601847e0cd/src/transformers/training_args.py\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './results',          # output directory\n",
    "    overwrite_output_dir = True,       # whether the contents in the output dir should be overwritten\n",
    "    do_train = True,                   # whether training should be run\n",
    "    do_eval = True,                    # whether evaluation should be run\n",
    "#     evaluate_during_training = True,   # run eval at each logging step - this throws errors\n",
    "    num_train_epochs = 2,              # total number of training epochs (2-4, rec)\n",
    "    per_device_train_batch_size = 8,   # batch size per device during training\n",
    "    per_device_eval_batch_size = 8,    # batch size per devive during eval\n",
    "    warmup_steps = 0,                  # number of warmup steps for learning rate scheduler\n",
    "    weight_decay = 0.01,               # strength of weight decay\n",
    "    logging_steps = 5000,              # controls how frequently progress is logged; default = 500\n",
    "    save_steps = 5000,                 # controls how frequently checkpoints are logged; default = 500\n",
    "    logging_dir = './logs',            # directory for storing logs\n",
    "    seed = 123                         # random seed\n",
    ")\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/a75c64d80c76c3dc71f735d9197a4a601847e0cd/src/transformers/trainer.py\n",
    "trainer = Trainer(\n",
    "    args = training_args,              # training arguments, defined above\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    "    train_dataset = datasetTrain, #datasetKMG, \n",
    "    eval_dataset = datasetEvaluate, # TrainOutput(global_step=60000, training_loss=1.7492684895833333)\n",
    "#     compute_metrics = compute_metrics,\n",
    "    model = model     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK]\n"
     ]
    }
   ],
   "source": [
    "# so the model trainer knows what the mask token is, good.\n",
    "print(trainer.tokenizer.mask_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fadd6a09ab5741e9af9b1e4beebc5c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=2.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35237ef2cc5b4472866b5920de06a0a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=10000.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.042169921875, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.5, 'total_flos': 433336842705888, 'step': 5000}\n",
      "{'loss': 2.4209765625, 'learning_rate': 2.5e-05, 'epoch': 1.0, 'total_flos': 867413188698624, 'step': 10000}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0cc16081f804ebf81060cc7aad87451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=10000.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.898679296875, 'learning_rate': 1.25e-05, 'epoch': 1.5, 'total_flos': 1303323792844032, 'step': 15000}\n",
      "{'loss': 1.69612890625, 'learning_rate': 0.0, 'epoch': 2.0, 'total_flos': 1735940465682336, 'step': 20000}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20000, training_loss=2.514488671875)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this does a pretty print-out of model architecture\n",
    "# # TODO: confirm that nothing is being left out of this print-out\n",
    "# params = list(model.named_parameters())\n",
    "\n",
    "# print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "# print('==== Embedding Layer ====\\n')\n",
    "# for p in params[0:5]:\n",
    "#     print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "# print('\\n==== First Transformer ====\\n')\n",
    "# for p in params[5:21]:\n",
    "#     print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "# print('\\n==== Output Layer ====\\n')\n",
    "# for p in params[-4:]:\n",
    "#     print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./telemed_Model/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-18ae7d2a4e56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# to use the fill-mask pipeline (and get reproducible results) we need to take model out of train mode and put in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# evaluation model. The only way I can find to do this is by using the \"from_pretrained\" method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodelTrained\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./telemed_Model/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# https://huggingface.co/transformers/migration.html\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# to use the fill-mask pipeline (and get reproducible results) we need to take model out of train mode and put in \n",
    "# evaluation model. The only way I can find to do this is by using the \"from_pretrained\" method\n",
    "modelTrained = model.from_pretrained('./telemed_Model/')\n",
    "\n",
    "# https://huggingface.co/transformers/migration.html\n",
    "# Models are now set in evaluation mode by default when instantiated with the from_pretrained() method. \n",
    "# To train them don’t forget to set them back in training mode (model.train()) to activate the dropout modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, FillMaskPipeline\n",
    "\n",
    "fill_mask = FillMaskPipeline(\n",
    "    model = modelTrained, \n",
    "    tokenizer = tokenizer,\n",
    "    topk = 10\n",
    ")\n",
    "\n",
    "fill_mask.save_pretrained('./telemed_Fill_Mask_Pipeline/') \n",
    "\n",
    "# print(fill_mask.tokenizer.mask_token) ## [MASK]\n",
    "# print(tokenizer.mask_token) ## [MASK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
