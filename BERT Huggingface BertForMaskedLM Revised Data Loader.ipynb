{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = \"C:\\Java\\jdk1.8.0_221\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install --upgrade --use-feature=2020-resolver torch \n",
    "# !{sys.executable} -m pip install --upgrade --use-feature=2020-resolver tensorflow\n",
    "# !{sys.executable} -m pip install --upgrade --use-feature=2020-resolver transformers \n",
    "# !{sys.executable} -m pip install --use-feature=2020-resolver tokenizers==0.8.1rc2\n",
    "# !{sys.executable} -m pip install --upgrade --use-feature=2020-resolver datasets\n",
    "# !{sys.executable} -m pip install --upgrade nltk\n",
    "# !{sys.executable} -m pip install bert_score\n",
    "# !{sys.executable} -m pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import csv \n",
    "\n",
    "import torch \n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DistilBertForMaskedLM, DistilBertConfig, DistilBertTokenizer\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "from datasets import  list_datasets, load_dataset, list_metrics, load_metric \n",
    "\n",
    "from nltk.tokenize import sent_tokenize # for splitting paragraphs into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS WORKAROUND FOR ISSUES WITH LOAD_DATASET\n",
    "import pandas as pd\n",
    "\n",
    "read_file = pd.read_csv(\n",
    "    './Desktop/telemed/data for Huggingface/huggingface_training_data.csv', \n",
    "    sep='\\n'\n",
    ")\n",
    "\n",
    "# convert text datatype to string\n",
    "read_file['text'] = read_file['text'].astype('string')\n",
    "\n",
    "# split the pandas data into separate sentences (from paragraphs)\n",
    "read_file['text'] = read_file['text'].apply(sent_tokenize)\n",
    "\n",
    "# then explode that dataframe so each sentence is in its own row\n",
    "read_file = read_file.explode('text')\n",
    "\n",
    "# convert text datatype to string\n",
    "read_file['text'] = read_file['text'].astype('string')\n",
    "\n",
    "# need to drop index or it created issues later\n",
    "read_file = read_file.reset_index(drop=True)\n",
    "\n",
    "# remove short strings\n",
    "read_file = read_file[read_file.text.str.len() > 1]\n",
    "\n",
    "# DELETE THIS - IT BAD\n",
    "read_file = read_file[0:300_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240000, 1)\n",
      "(60000, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, evaluate = train_test_split(\n",
    "    read_file, \n",
    "    train_size=0.8, \n",
    "    random_state = 123\n",
    ")\n",
    "\n",
    "print(train.shape)\n",
    "print(evaluate.shape)\n",
    "\n",
    "# write training data and evaluation data to csv\n",
    "train.to_csv(\n",
    "    './telemed_Data_Train/sentence_data.csv', \n",
    "    index = False, \n",
    "    columns = ['text'],\n",
    "    header = False,\n",
    "    quoting = csv.QUOTE_NONE,\n",
    "    escapechar = '\\\\'\n",
    ")\n",
    "\n",
    "evaluate.to_csv(\n",
    "    './telemed_Data_Evaluate/sentence_data.csv', \n",
    "    index = False, \n",
    "    columns = ['text'],\n",
    "    header = False,\n",
    "    quoting = csv.QUOTE_NONE,\n",
    "    escapechar = '\\\\'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the file with each sentence on a separate line to csv\n",
    "read_file.to_csv(\n",
    "    './telemed_Data/sentence_data.csv', \n",
    "    index = False, \n",
    "    columns = ['text'],\n",
    "    header = False,\n",
    "    quoting = csv.QUOTE_NONE,\n",
    "    escapechar = '\\\\'\n",
    "#     quoting = csv.QUOTE_ALL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./telemed_Vocab2/telemed-bert-wordpiece-vocab.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create vocab file....\n",
    "# paths = [str(x) for x in Path(\"./Desktop/telemed/data for Huggingface/\").glob(\"**/*.csv\")]\n",
    "paths = [str(x) for x in Path(\"./telemed_Data/\").glob(\"**/*.csv\")]\n",
    "tokenizer =  BertWordPieceTokenizer()\n",
    "\n",
    "tokenizer.enable_truncation(max_length = 512)\n",
    "tokenizer.train(files = paths, \n",
    "                vocab_size = 4000, \n",
    "                min_frequency = 2,\n",
    "                special_tokens = ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']\n",
    "               )\n",
    "tokenizer.save_model(\"./telemed_Vocab2/\", name = 'telemed-bert-wordpiece')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer('./telemed_Vocab2/telemed-bert-wordpiece-vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# tell pytorch what kind of processor is available to it\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.data.datasets.language_modeling.LineByLineTextDataset'>\n"
     ]
    }
   ],
   "source": [
    "# Is there a simpler way to build training dataset?\n",
    "datasetKMG = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"./telemed_Data/sentence_data.csv\",\n",
    "    block_size=128, # HOLY COW is the training fast! ... and block_size is max length of vector...so maybe dial up to 512? (but sentence len > 512? Not likely here.)\n",
    ")\n",
    "\n",
    "print(type(datasetKMG))\n",
    "# https://github.com/huggingface/transformers/blob/762cba3bdaf70104dc17cc7ff0f8ce13ba23d558/src/transformers/data/datasets/language_modeling.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build training dataset\n",
    "datasetTrain = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"./telemed_Data_Train/sentence_data.csv\",\n",
    "    block_size=128, # block_size is max length of vector...so maybe dial up to 512? (but sentence len > 512? Not likely here.)\n",
    ")\n",
    "\n",
    "# build evaluation dataset\n",
    "datasetEvaluate = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"./telemed_Data_Evaluate/sentence_data.csv\",\n",
    "    block_size=128, # block_size is max length of vector...so maybe dial up to 512? (but sentence len > 512? Not likely here.)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicitly create data collator instead of letting it get implicitly created by trainer\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer = tokenizer, \n",
    "    mlm = True, \n",
    "    mlm_probability = 0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 308, 28, 270, 21, 96, 36, 14, 352, 28, 21, 451, 137, 455, 16, 3]\n",
      "[2, 304, 28, 168, 123, 1692, 1493, 687, 461, 530, 839, 133, 959, 1021, 16, 3]\n",
      "[2, 111, 3022, 128, 133, 3390, 128, 1137, 1657, 137, 320, 1581, 3825, 124, 133, 320, 768, 38, 1079, 942, 3162, 1417, 16, 3]\n",
      "[2, 111, 1993, 465, 133, 2820, 885, 122, 111, 896, 766, 36, 14, 1560, 133, 1721, 525, 16, 3]\n"
     ]
    }
   ],
   "source": [
    "print(datasetKMG.examples[0])\n",
    "print(datasetKMG.examples[1])\n",
    "print(datasetKMG.examples[2]) # No padding. But this can still get run through training\n",
    "print(datasetKMG.examples[50003])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] abdomen : may 31 \\, 2020 : 3 images are provided. [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(datasetKMG.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user can define their own custom metrics \n",
    "# UNDER DEVELOPMENT - NOT SURE IF THIS WILL WORK AND WHAT THE LABELS ARE IN THIS CONTEXT\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We might need to use Trainer and TrainingArguments to train FROM SCRATCH!\n",
    "# https://github.com/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb\n",
    "\n",
    "config = DistilBertConfig(\n",
    "    vocab_size = tokenizer.vocab_size, # this is the only default I'm changing\n",
    "    max_position_embeddings=512, \n",
    "    sinusoidal_pos_embds=False, \n",
    "    n_layers=6, \n",
    "    n_heads=12, \n",
    "    dim=768, \n",
    "    hidden_dim=3072, \n",
    "    dropout=0.1, \n",
    "    attention_dropout=0.1, \n",
    "    activation='gelu', \n",
    "    initializer_range=0.02, \n",
    "    qa_dropout=0.1, \n",
    "    seq_classif_dropout=0.2, \n",
    "    pad_token_id=0\n",
    ")\n",
    "\n",
    "model = DistilBertForMaskedLM(config)\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/a75c64d80c76c3dc71f735d9197a4a601847e0cd/src/transformers/training_args.py\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './results',          # output directory\n",
    "    overwrite_output_dir = True,       # whether the contents in the output dir should be overwritten\n",
    "    do_train = True,                   # whether training should be run\n",
    "    do_eval = True,                    # whether evaluation should be run\n",
    "#     evaluate_during_training = True,   # run eval at each logging step - this throws errors\n",
    "    num_train_epochs = 2,              # total number of training epochs (2-4, rec)\n",
    "    per_device_train_batch_size = 8,   # batch size per device during training\n",
    "    per_device_eval_batch_size = 8,    # batch size per devive during eval\n",
    "    warmup_steps = 0,                  # number of warmup steps for learning rate scheduler\n",
    "    weight_decay = 0.01,               # strength of weight decay\n",
    "    logging_dir = './logs',            # directory for storing logs\n",
    "    seed = 123                         # random seed\n",
    ")\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/a75c64d80c76c3dc71f735d9197a4a601847e0cd/src/transformers/trainer.py\n",
    "trainer = Trainer(\n",
    "    args = training_args,              # training arguments, defined above\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    "    train_dataset = datasetTrain, #datasetKMG, \n",
    "    eval_dataset = datasetEvaluate,\n",
    "#     compute_metrics = compute_metrics,\n",
    "    model = model     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK]\n"
     ]
    }
   ],
   "source": [
    "# so the model trainer knows what the mask token is, good.\n",
    "print(trainer.tokenizer.mask_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f91ec8a7e24b809f0ef8601236acbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=2.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9c7059be834899acfe91c719f72faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=30000.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.91733349609375, 'learning_rate': 4.958333333333334e-05, 'epoch': 0.016666666666666666, 'step': 500}\n",
      "{'loss': 5.24743212890625, 'learning_rate': 4.9166666666666665e-05, 'epoch': 0.03333333333333333, 'step': 1000}\n",
      "{'loss': 4.8485107421875, 'learning_rate': 4.875e-05, 'epoch': 0.05, 'step': 1500}\n",
      "{'loss': 4.5343349609375, 'learning_rate': 4.8333333333333334e-05, 'epoch': 0.06666666666666667, 'step': 2000}\n",
      "{'loss': 4.22444140625, 'learning_rate': 4.791666666666667e-05, 'epoch': 0.08333333333333333, 'step': 2500}\n",
      "{'loss': 3.9406640625, 'learning_rate': 4.75e-05, 'epoch': 0.1, 'step': 3000}\n",
      "{'loss': 3.637568359375, 'learning_rate': 4.708333333333334e-05, 'epoch': 0.11666666666666667, 'step': 3500}\n",
      "{'loss': 3.45575, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.13333333333333333, 'step': 4000}\n",
      "{'loss': 3.2852890625, 'learning_rate': 4.6250000000000006e-05, 'epoch': 0.15, 'step': 4500}\n",
      "{'loss': 3.150328125, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.16666666666666666, 'step': 5000}\n",
      "{'loss': 2.98941015625, 'learning_rate': 4.541666666666667e-05, 'epoch': 0.18333333333333332, 'step': 5500}\n",
      "{'loss': 2.89725390625, 'learning_rate': 4.5e-05, 'epoch': 0.2, 'step': 6000}\n",
      "{'loss': 2.8131484375, 'learning_rate': 4.458333333333334e-05, 'epoch': 0.21666666666666667, 'step': 6500}\n",
      "{'loss': 2.68316015625, 'learning_rate': 4.4166666666666665e-05, 'epoch': 0.23333333333333334, 'step': 7000}\n",
      "{'loss': 2.56371484375, 'learning_rate': 4.375e-05, 'epoch': 0.25, 'step': 7500}\n",
      "{'loss': 2.503890625, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.26666666666666666, 'step': 8000}\n",
      "{'loss': 2.45012890625, 'learning_rate': 4.291666666666667e-05, 'epoch': 0.2833333333333333, 'step': 8500}\n",
      "{'loss': 2.3910625, 'learning_rate': 4.25e-05, 'epoch': 0.3, 'step': 9000}\n",
      "{'loss': 2.354, 'learning_rate': 4.208333333333334e-05, 'epoch': 0.31666666666666665, 'step': 9500}\n",
      "{'loss': 2.2906171875, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.3333333333333333, 'step': 10000}\n",
      "{'loss': 2.222140625, 'learning_rate': 4.125e-05, 'epoch': 0.35, 'step': 10500}\n",
      "{'loss': 2.1728984375, 'learning_rate': 4.0833333333333334e-05, 'epoch': 0.36666666666666664, 'step': 11000}\n",
      "{'loss': 2.187828125, 'learning_rate': 4.041666666666667e-05, 'epoch': 0.38333333333333336, 'step': 11500}\n",
      "{'loss': 2.08003125, 'learning_rate': 4e-05, 'epoch': 0.4, 'step': 12000}\n",
      "{'loss': 2.05634375, 'learning_rate': 3.958333333333333e-05, 'epoch': 0.4166666666666667, 'step': 12500}\n",
      "{'loss': 1.9767421875, 'learning_rate': 3.9166666666666665e-05, 'epoch': 0.43333333333333335, 'step': 13000}\n",
      "{'loss': 1.9443125, 'learning_rate': 3.875e-05, 'epoch': 0.45, 'step': 13500}\n",
      "{'loss': 2.0258125, 'learning_rate': 3.8333333333333334e-05, 'epoch': 0.4666666666666667, 'step': 14000}\n",
      "{'loss': 1.9000546875, 'learning_rate': 3.791666666666667e-05, 'epoch': 0.48333333333333334, 'step': 14500}\n",
      "{'loss': 1.883015625, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.5, 'step': 15000}\n",
      "{'loss': 1.8845234375, 'learning_rate': 3.708333333333334e-05, 'epoch': 0.5166666666666667, 'step': 15500}\n",
      "{'loss': 1.846953125, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.5333333333333333, 'step': 16000}\n",
      "{'loss': 1.8591796875, 'learning_rate': 3.625e-05, 'epoch': 0.55, 'step': 16500}\n",
      "{'loss': 1.81490625, 'learning_rate': 3.5833333333333335e-05, 'epoch': 0.5666666666666667, 'step': 17000}\n",
      "{'loss': 1.760734375, 'learning_rate': 3.541666666666667e-05, 'epoch': 0.5833333333333334, 'step': 17500}\n",
      "{'loss': 1.709140625, 'learning_rate': 3.5e-05, 'epoch': 0.6, 'step': 18000}\n",
      "{'loss': 1.7127421875, 'learning_rate': 3.458333333333333e-05, 'epoch': 0.6166666666666667, 'step': 18500}\n",
      "{'loss': 1.7557421875, 'learning_rate': 3.4166666666666666e-05, 'epoch': 0.6333333333333333, 'step': 19000}\n",
      "{'loss': 1.7151953125, 'learning_rate': 3.375000000000001e-05, 'epoch': 0.65, 'step': 19500}\n",
      "{'loss': 1.7107421875, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.6666666666666666, 'step': 20000}\n",
      "{'loss': 1.6733125, 'learning_rate': 3.291666666666667e-05, 'epoch': 0.6833333333333333, 'step': 20500}\n",
      "{'loss': 1.632671875, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.7, 'step': 21000}\n",
      "{'loss': 1.6414453125, 'learning_rate': 3.208333333333334e-05, 'epoch': 0.7166666666666667, 'step': 21500}\n",
      "{'loss': 1.638171875, 'learning_rate': 3.1666666666666666e-05, 'epoch': 0.7333333333333333, 'step': 22000}\n",
      "{'loss': 1.6365859375, 'learning_rate': 3.125e-05, 'epoch': 0.75, 'step': 22500}\n",
      "{'loss': 1.5966328125, 'learning_rate': 3.0833333333333335e-05, 'epoch': 0.7666666666666667, 'step': 23000}\n",
      "{'loss': 1.5413515625, 'learning_rate': 3.0416666666666666e-05, 'epoch': 0.7833333333333333, 'step': 23500}\n",
      "{'loss': 1.6129609375, 'learning_rate': 3e-05, 'epoch': 0.8, 'step': 24000}\n",
      "{'loss': 1.51209375, 'learning_rate': 2.9583333333333335e-05, 'epoch': 0.8166666666666667, 'step': 24500}\n",
      "{'loss': 1.5000078125, 'learning_rate': 2.916666666666667e-05, 'epoch': 0.8333333333333334, 'step': 25000}\n",
      "{'loss': 1.509625, 'learning_rate': 2.8749999999999997e-05, 'epoch': 0.85, 'step': 25500}\n",
      "{'loss': 1.5033671875, 'learning_rate': 2.8333333333333335e-05, 'epoch': 0.8666666666666667, 'step': 26000}\n",
      "{'loss': 1.514421875, 'learning_rate': 2.791666666666667e-05, 'epoch': 0.8833333333333333, 'step': 26500}\n",
      "{'loss': 1.48321875, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.9, 'step': 27000}\n",
      "{'loss': 1.4405234375, 'learning_rate': 2.7083333333333332e-05, 'epoch': 0.9166666666666666, 'step': 27500}\n",
      "{'loss': 1.493, 'learning_rate': 2.6666666666666667e-05, 'epoch': 0.9333333333333333, 'step': 28000}\n",
      "{'loss': 1.4676875, 'learning_rate': 2.625e-05, 'epoch': 0.95, 'step': 28500}\n",
      "{'loss': 1.489953125, 'learning_rate': 2.5833333333333336e-05, 'epoch': 0.9666666666666667, 'step': 29000}\n",
      "{'loss': 1.42896875, 'learning_rate': 2.5416666666666667e-05, 'epoch': 0.9833333333333333, 'step': 29500}\n",
      "{'loss': 1.43234375, 'learning_rate': 2.5e-05, 'epoch': 1.0, 'step': 30000}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa6027c3cb44adea88a8dfec85f80a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=30000.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.397015625, 'learning_rate': 2.4583333333333332e-05, 'epoch': 1.0166666666666666, 'step': 30500}\n",
      "{'loss': 1.358109375, 'learning_rate': 2.4166666666666667e-05, 'epoch': 1.0333333333333334, 'step': 31000}\n",
      "{'loss': 1.358234375, 'learning_rate': 2.375e-05, 'epoch': 1.05, 'step': 31500}\n",
      "{'loss': 1.37621875, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.0666666666666667, 'step': 32000}\n",
      "{'loss': 1.343390625, 'learning_rate': 2.2916666666666667e-05, 'epoch': 1.0833333333333333, 'step': 32500}\n",
      "{'loss': 1.35740625, 'learning_rate': 2.25e-05, 'epoch': 1.1, 'step': 33000}\n",
      "{'loss': 1.32590625, 'learning_rate': 2.2083333333333333e-05, 'epoch': 1.1166666666666667, 'step': 33500}\n",
      "{'loss': 1.3435, 'learning_rate': 2.1666666666666667e-05, 'epoch': 1.1333333333333333, 'step': 34000}\n",
      "{'loss': 1.304296875, 'learning_rate': 2.125e-05, 'epoch': 1.15, 'step': 34500}\n",
      "{'loss': 1.350984375, 'learning_rate': 2.0833333333333336e-05, 'epoch': 1.1666666666666667, 'step': 35000}\n",
      "{'loss': 1.311734375, 'learning_rate': 2.0416666666666667e-05, 'epoch': 1.1833333333333333, 'step': 35500}\n",
      "{'loss': 1.311703125, 'learning_rate': 2e-05, 'epoch': 1.2, 'step': 36000}\n",
      "{'loss': 1.281765625, 'learning_rate': 1.9583333333333333e-05, 'epoch': 1.2166666666666668, 'step': 36500}\n",
      "{'loss': 1.32540625, 'learning_rate': 1.9166666666666667e-05, 'epoch': 1.2333333333333334, 'step': 37000}\n",
      "{'loss': 1.290171875, 'learning_rate': 1.8750000000000002e-05, 'epoch': 1.25, 'step': 37500}\n",
      "{'loss': 1.277015625, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.2666666666666666, 'step': 38000}\n",
      "{'loss': 1.302421875, 'learning_rate': 1.7916666666666667e-05, 'epoch': 1.2833333333333332, 'step': 38500}\n",
      "{'loss': 1.252109375, 'learning_rate': 1.75e-05, 'epoch': 1.3, 'step': 39000}\n",
      "{'loss': 1.269453125, 'learning_rate': 1.7083333333333333e-05, 'epoch': 1.3166666666666667, 'step': 39500}\n",
      "{'loss': 1.23521875, 'learning_rate': 1.6666666666666667e-05, 'epoch': 1.3333333333333333, 'step': 40000}\n",
      "{'loss': 1.264625, 'learning_rate': 1.6250000000000002e-05, 'epoch': 1.35, 'step': 40500}\n",
      "{'loss': 1.23759375, 'learning_rate': 1.5833333333333333e-05, 'epoch': 1.3666666666666667, 'step': 41000}\n",
      "{'loss': 1.26375, 'learning_rate': 1.5416666666666668e-05, 'epoch': 1.3833333333333333, 'step': 41500}\n",
      "{'loss': 1.259359375, 'learning_rate': 1.5e-05, 'epoch': 1.4, 'step': 42000}\n",
      "{'loss': 1.24603125, 'learning_rate': 1.4583333333333335e-05, 'epoch': 1.4166666666666667, 'step': 42500}\n",
      "{'loss': 1.2029375, 'learning_rate': 1.4166666666666668e-05, 'epoch': 1.4333333333333333, 'step': 43000}\n",
      "{'loss': 1.246921875, 'learning_rate': 1.3750000000000002e-05, 'epoch': 1.45, 'step': 43500}\n",
      "{'loss': 1.185390625, 'learning_rate': 1.3333333333333333e-05, 'epoch': 1.4666666666666668, 'step': 44000}\n",
      "{'loss': 1.23409375, 'learning_rate': 1.2916666666666668e-05, 'epoch': 1.4833333333333334, 'step': 44500}\n",
      "{'loss': 1.196359375, 'learning_rate': 1.25e-05, 'epoch': 1.5, 'step': 45000}\n",
      "{'loss': 1.1766875, 'learning_rate': 1.2083333333333333e-05, 'epoch': 1.5166666666666666, 'step': 45500}\n",
      "{'loss': 1.198546875, 'learning_rate': 1.1666666666666668e-05, 'epoch': 1.5333333333333332, 'step': 46000}\n",
      "{'loss': 1.1380625, 'learning_rate': 1.125e-05, 'epoch': 1.55, 'step': 46500}\n",
      "{'loss': 1.210203125, 'learning_rate': 1.0833333333333334e-05, 'epoch': 1.5666666666666667, 'step': 47000}\n",
      "{'loss': 1.202921875, 'learning_rate': 1.0416666666666668e-05, 'epoch': 1.5833333333333335, 'step': 47500}\n",
      "{'loss': 1.166390625, 'learning_rate': 1e-05, 'epoch': 1.6, 'step': 48000}\n",
      "{'loss': 1.213203125, 'learning_rate': 9.583333333333334e-06, 'epoch': 1.6166666666666667, 'step': 48500}\n",
      "{'loss': 1.151578125, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.6333333333333333, 'step': 49000}\n",
      "{'loss': 1.125953125, 'learning_rate': 8.75e-06, 'epoch': 1.65, 'step': 49500}\n",
      "{'loss': 1.11903125, 'learning_rate': 8.333333333333334e-06, 'epoch': 1.6666666666666665, 'step': 50000}\n",
      "{'loss': 1.141703125, 'learning_rate': 7.916666666666667e-06, 'epoch': 1.6833333333333333, 'step': 50500}\n",
      "{'loss': 1.110625, 'learning_rate': 7.5e-06, 'epoch': 1.7, 'step': 51000}\n",
      "{'loss': 1.12040625, 'learning_rate': 7.083333333333334e-06, 'epoch': 1.7166666666666668, 'step': 51500}\n",
      "{'loss': 1.143234375, 'learning_rate': 6.666666666666667e-06, 'epoch': 1.7333333333333334, 'step': 52000}\n",
      "{'loss': 1.1316875, 'learning_rate': 6.25e-06, 'epoch': 1.75, 'step': 52500}\n",
      "{'loss': 1.13834375, 'learning_rate': 5.833333333333334e-06, 'epoch': 1.7666666666666666, 'step': 53000}\n",
      "{'loss': 1.102515625, 'learning_rate': 5.416666666666667e-06, 'epoch': 1.7833333333333332, 'step': 53500}\n",
      "{'loss': 1.12175, 'learning_rate': 5e-06, 'epoch': 1.8, 'step': 54000}\n",
      "{'loss': 1.10315625, 'learning_rate': 4.583333333333333e-06, 'epoch': 1.8166666666666667, 'step': 54500}\n",
      "{'loss': 1.110015625, 'learning_rate': 4.166666666666667e-06, 'epoch': 1.8333333333333335, 'step': 55000}\n",
      "{'loss': 1.097828125, 'learning_rate': 3.75e-06, 'epoch': 1.85, 'step': 55500}\n",
      "{'loss': 1.1224375, 'learning_rate': 3.3333333333333333e-06, 'epoch': 1.8666666666666667, 'step': 56000}\n",
      "{'loss': 1.1629375, 'learning_rate': 2.916666666666667e-06, 'epoch': 1.8833333333333333, 'step': 56500}\n",
      "{'loss': 1.141765625, 'learning_rate': 2.5e-06, 'epoch': 1.9, 'step': 57000}\n",
      "{'loss': 1.113515625, 'learning_rate': 2.0833333333333334e-06, 'epoch': 1.9166666666666665, 'step': 57500}\n",
      "{'loss': 1.119921875, 'learning_rate': 1.6666666666666667e-06, 'epoch': 1.9333333333333333, 'step': 58000}\n",
      "{'loss': 1.126046875, 'learning_rate': 1.25e-06, 'epoch': 1.95, 'step': 58500}\n",
      "{'loss': 1.1058125, 'learning_rate': 8.333333333333333e-07, 'epoch': 1.9666666666666668, 'step': 59000}\n",
      "{'loss': 1.06296875, 'learning_rate': 4.1666666666666667e-07, 'epoch': 1.9833333333333334, 'step': 59500}\n",
      "{'loss': 1.078421875, 'learning_rate': 0.0, 'epoch': 2.0, 'step': 60000}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60000, training_loss=1.7492684895833333)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this does a pretty print-out of model architecture\n",
    "# TODO: confirm that nothing is being left out of this print-out\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "print('==== Embedding Layer ====\\n')\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./telemed_Model/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use the fill-mask pipeline (and get reproducible results) we need to take model out of train mode and put in \n",
    "# evaluation model. The only way I can find to do this is by using the \"from_pretrained\" method\n",
    "modelTrained = model.from_pretrained('./telemed_Model2/')\n",
    "\n",
    "# https://huggingface.co/transformers/migration.html\n",
    "# Models are now set in evaluation mode by default when instantiated with the from_pretrained() method. \n",
    "# To train them don’t forget to set them back in training mode (model.train()) to activate the dropout modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, FillMaskPipeline\n",
    "\n",
    "fill_mask = FillMaskPipeline(\n",
    "    model = modelTrained, \n",
    "    tokenizer = tokenizer,\n",
    "    topk = 10\n",
    ")\n",
    "\n",
    "# print(fill_mask.tokenizer.mask_token) ## [MASK]\n",
    "# print(tokenizer.mask_token) ## [MASK]\n",
    "\n",
    "# There are no appreciable mineral opaque calculi within the urinary bladder on the available study.\n",
    "sequence = f\"There are no appreciable mineral opaque {tokenizer.mask_token} within the urinary bladder on the available study.\"\n",
    "top_k = fill_mask(sequence)\n",
    "\n",
    "print(\"Complete sentence: There are no appreciable mineral opaque calculi within the urinary bladder on the available study.\")\n",
    "print(\"\\n\")\n",
    "print(\"Sentence with exactly one token masked:\")\n",
    "print(sequence)\n",
    "print(\"\\n\")\n",
    "print(\"k sentences with masked token filled and likelihood (probability?) of that token:\")\n",
    "for item in top_k:\n",
    "    print(item['sequence'])\n",
    "    print(item['score'])\n",
    "#     print(item['token'])\n",
    "#     print(item['token_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try another sentence \n",
    "# sequence = f\"There is a mild diffuse {tokenizer.mask_token} she will long pattern, the interstitial component of which is accentuated by expiratory phase of respiration.\"\n",
    "sequence = f\"There is a mild diffuse {tokenizer.mask_token} she will long pattern, the interstitial component of which is accentuated by expiratory phase of respiration.\" \n",
    "top_k = fill_mask(sequence)\n",
    "\n",
    "print(\"Complete sentence: There is a mild diffuse broncho-interspace she will long pattern, the interstitial component of which is accentuated by expiratory phase of respiration.\")\n",
    "print(\"\\n\")\n",
    "print(\"Sentence with exactly one token masked:\")\n",
    "print(sequence)\n",
    "print(\"\\n\")\n",
    "print(\"k sentences with masked token filled and likelihood (probability?) of that token:\")\n",
    "for item in top_k:\n",
    "    print(item['sequence'])\n",
    "    print(item['score'])\n",
    "    \n",
    "# parenchymal is an adjective!!\n",
    "# parenchyma is a noun - model knows a noun belongs in that slot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what kinds of performance metrics come built-in?\n",
    "# print(list_metrics())\n",
    "\n",
    "# BERTScore leverages the pre-trained contextual embeddings from BERT and matches words in candidate and \n",
    "# reference sentences by cosine similarity. It has been shown to correlate with human judgment on sentence-level and \n",
    "# system-level evaluation. Moreover, BERTScore computes precision, recall, and F1 measure, which can be useful for \n",
    "# evaluating different language generation tasks.\n",
    "# https://arxiv.org/pdf/1904.09675.pdf --- shows how BERT score is calculated\n",
    "metric = load_metric(\"bertscore\")\n",
    "\n",
    "# Example of typical usage\n",
    "for batch in read_file['text'][1:2]:\n",
    "    inputs = torch.tensor(tokenizer.encode(batch, add_special_tokens = True)).unsqueeze(0)\n",
    "    predictions = modelTrained(input_ids = inputs)[0]\n",
    "    predicted_index = torch.argmax(input = predictions[0], dim = 1)\n",
    "    predicted_text = tokenizer.decode(predicted_index)\n",
    "    print(\"the original text:\")\n",
    "    print(batch)\n",
    "    print('\\n')\n",
    "    print(\"input_ids for the original text:\")\n",
    "    print(inputs)\n",
    "    print('\\n')\n",
    "    print(\"m x n predictions matrix: m is number of tokens in sentence; n is dimension of vocab\")\n",
    "    print(predictions)\n",
    "    print(predictions.shape)\n",
    "    print('\\n')\n",
    "    print(\"these are the most likely tokens (ids) for each position in the sentence\")\n",
    "    print(predicted_index)\n",
    "    print('\\n')\n",
    "    print(\"these are the most likely token ids converted to text\")\n",
    "    print(predicted_text)\n",
    "#     metric.add_batch(predictions=[\"I like to take long walks.\"], references=[\"I like to take lengthy walks.\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = metric.compute(\n",
    "#     predictions = [\"I like to take lengthy walks.\", \"I live in a horse\"], \n",
    "#     references = [\"I like to take long walks.\", \"I live in a house.\"],\n",
    "#     lang = 'en'\n",
    "# )\n",
    "# print(score)\n",
    "\n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(DataCollatorForLanguageModeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
